{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T15:43:48.576093Z",
     "start_time": "2025-05-14T15:43:46.789736Z"
    }
   },
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. 编码器\n",
    "\n",
    "### 思路讲解：\n",
    "**1. Embedding层**\n",
    "- 把原始的“单词编号”变成“稠密向量”，让模型能感知单词之间的语义距离。就像从“傻瓜编号”变成“词语地图”。\n",
    "\n",
    "**2. GRU堆叠**\n",
    "- 输入嵌入序列，经过多层GRU，提取输入句子的“时序特征”。堆叠多层=模型更聪明。\n",
    "- 可以设定层数，加入dropout（丢弃法，防过拟合）。\n",
    "\n",
    "**3. 前向传播**\n",
    "- 先embedding，再调整tensor维度：PyTorch里RNN输入要时间步优先（steps, batch, embed）。\n",
    "- 最后扔进GRU，得到：\n",
    "    - output：每个时刻的隐藏状态（可用于注意力机制等）\n",
    "    - state：每层最后一步的隐藏状态（通常给decoder）\n",
    "\n",
    "**4. 核心思想**\n",
    "- 编码器就是把一句话“压缩”为有用的信息，供后续解码器解读。\n",
    "- RNN（GRU/LSTM）负责捕捉“顺序和上下文”。\n",
    "- 输出的状态代表“读完这句话，我记住了啥”。"
   ],
   "id": "ef64e55d7b2cda96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:07.177418Z",
     "start_time": "2025-05-14T15:44:07.171222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Encoder'''\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 单词索引->稠密向量（嵌入层，学到单词之间的联系）\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 堆叠GRU：多个GRU层叠加，能学到更复杂的时序特征\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X: (batch, steps) 例：[32, 10]\n",
    "        X = self.embedding(X)            # (batch, steps, embed) 例：[32, 10, 16]\n",
    "        X = X.permute(1, 0, 2)           # PyTorch的RNN喜欢时间步优先：(steps, batch, embed) 例：[10, 32, 16]\n",
    "        output, state = self.rnn(X)      # output: 所有时刻的隐状态  (steps, batch, hidden)\n",
    "                                         # state: 每层最后的隐状态 (num_layers, batch, hidden)\n",
    "        return output, state\n"
   ],
   "id": "e74ba85fe72b11a3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:17.693792Z",
     "start_time": "2025-05-14T15:44:17.680523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ],
   "id": "23885a347ab822b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:19.261109Z",
     "start_time": "2025-05-14T15:44:19.257636Z"
    }
   },
   "cell_type": "code",
   "source": "state.shape",
   "id": "c15dbc6c9ba2711b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. 解码器\n",
    "\n",
    "### 实现思路\n",
    "**1. Decoder输入拼接context**\n",
    "- 和encoder不一样，decoder每一步输入除了token embedding，还要加一个“全局context”——也就是encoder最后一层隐状态。这让每个解码步都能直接获得整句话信息。\n",
    "- 实现：把state[-1]复制到每个时间步，然后拼接到input embedding后面。\n",
    "\n",
    "**2. forward过程**\n",
    "- X先embedding、然后调成(steps, batch, embed)。\n",
    "- 拼上context，输入给GRU。\n",
    "- Linear层把GRU输出变成预测下一个词的概率分布。\n",
    "\n",
    "**3. init_state**\n",
    "- 直接用encoder的最终隐藏状态作为decoder的初始隐状态——这样“翻译”能顺利衔接。\n",
    "\n",
    "**4. 输出形状说明**\n",
    "- output: (batch, steps, vocab_size) —— 每个解码步预测所有词概率。\n",
    "- state: 传递到下一个时刻的隐状态。\n",
    "\n",
    "### Embedding是什么\n",
    "* **Embedding**：泛指“把离散的对象（如单词、token）转成稠密向量”，常用于神经网络输入。\n",
    "* **Token Embedding**：就是把“token”（可以是单词、子词、字符等）转成稠密向量，token是什么就embed什么。\n",
    "* **Word Embedding**：特指“单词级别”的embedding，即把单词变成向量，比如Word2Vec、GloVe。“Word embedding”专指token=单词的情况。\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "66253682f308c591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:57.950989Z",
     "start_time": "2025-05-14T15:44:57.944810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Decoder '''\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 解码器输入是(嵌入向量 + 上文context)，所以输入维度加大\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)   # 输出层，预测下一个词的概率分布\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        # 只用encoder输出的最后状态（state）初始化decoder的隐状态\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X: (batch, steps) -> embedding + 交换轴 (steps, batch, embed)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "\n",
    "        # 拿encoder最后一层的隐状态，repeat到每个解码步\n",
    "        # context: (steps, batch, num_hiddens)\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "\n",
    "        # 拼接输入和context向量（在特征维上拼，搞成[embedding + context]）\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "\n",
    "        # 送进GRU，得到新的output和state\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "\n",
    "        # output: (steps, batch, num_hiddens) -> 经过Linear变为(vocab分布) -> 变成(batch, steps, vocab)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        return output, state\n",
    "\n"
   ],
   "id": "39be9e391b7446a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:03.796935Z",
     "start_time": "2025-05-14T15:45:03.784017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ],
   "id": "75395f5b6c00c605",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. 损失函数\n",
    "\n",
    "* 用于处理变长序列（比如有的句子7个词，有的10个），让padding部分不会参与计算。"
   ],
   "id": "90a6605d5332a5bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:33.218769Z",
     "start_time": "2025-05-14T15:45:33.208335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项（padding的位置填value）\"\"\"\n",
    "    maxlen = X.size(1)  # 序列最大长度\n",
    "    # mask: (batch, maxlen) 每个位置判断是否小于有效长度\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    # 将无效部分（padding部分）赋值为value（通常为0）\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ],
   "id": "b761c3cc5e6d58b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:38.381711Z",
     "start_time": "2025-05-14T15:45:38.367403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ],
   "id": "57e9ac7c30c319fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* 只对非padding部分计算loss，避免无效的padding影响模型学习。\n",
    "* 先用mask确定有效部分，然后loss按mask加权，最后按句子求均值。"
   ],
   "id": "10b5a33a7792a758"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:44.342052Z",
     "start_time": "2025-05-14T15:45:44.339154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失（只对有效token算loss）\"\"\"\n",
    "    # pred: (batch, steps, vocab)   label: (batch, steps)   valid_len: (batch, )\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)                    # 先全为1\n",
    "        weights = sequence_mask(weights, valid_len)         # 只有有效token为1，padding为0\n",
    "        self.reduction='none'                              # 不聚合，逐步输出loss\n",
    "        # 交叉熵：输入(batch, vocab, steps)和(batch, steps)\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1) # 只对有效部分求平均\n",
    "        return weighted_loss\n"
   ],
   "id": "8d21253697bd649d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:49.499545Z",
     "start_time": "2025-05-14T15:45:49.489805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ],
   "id": "e9d127ff4f9974ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. 训练",
   "id": "73b9e6ba2971e251"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:06.007039Z",
     "start_time": "2025-05-14T15:46:05.997243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练seq2seq模型\"\"\"\n",
    "\n",
    "    # ----------- 参数初始化 -----------\n",
    "    def xavier_init_weights(m):\n",
    "        # 用Xavier方法初始化Linear和GRU权重，防止梯度消失/爆炸\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)                                 # 模型丢到GPU/CPU\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)  # 优化器\n",
    "    loss = MaskedSoftmaxCELoss()                   # 损失函数（遮蔽padding）\n",
    "    net.train()                                    # 训练模式\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs])\n",
    "\n",
    "    # ----------- 训练主循环 -----------\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 记录：总损失/总token数\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()     # 梯度清零\n",
    "            # 数据批次搬到设备上\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            # 处理Decoder输入，加上<BOS>（强制教学：前一时刻真实输出作为当前输入）\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # [BOS, y1, y2,...]\n",
    "            # 模型前向传播\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            # 计算损失\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()        # 损失反向传播（sum成标量）\n",
    "            d2l.grad_clipping(net, 1) # 梯度裁剪（防爆炸）\n",
    "            num_tokens = Y_valid_len.sum()  # 本批次总token数\n",
    "            optimizer.step()          # 参数更新\n",
    "\n",
    "            # 记录损失和token数\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "\n",
    "        # 画loss曲线\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')\n"
   ],
   "id": "ab5a4459e1c1c244",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ],
   "id": "433881283032f570",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-05-14T23:46:36.526765</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 262.1875 183.35625 \nL 262.1875 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 145.8 \nL 245.44375 145.8 \nL 245.44375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 77.081681 145.8 \nL 77.081681 7.2 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"mc28a9c94be\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mc28a9c94be\" x=\"77.081681\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(70.719181 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 110.754095 145.8 \nL 110.754095 7.2 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mc28a9c94be\" x=\"110.754095\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(101.210345 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 144.426509 145.8 \nL 144.426509 7.2 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mc28a9c94be\" x=\"144.426509\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(134.882759 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 178.098922 145.8 \nL 178.098922 7.2 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mc28a9c94be\" x=\"178.098922\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(168.555172 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 211.771336 145.8 \nL 211.771336 7.2 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mc28a9c94be\" x=\"211.771336\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(202.227586 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 245.44375 145.8 \nL 245.44375 7.2 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mc28a9c94be\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(235.9 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(132.565625 174.076563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path d=\"M 50.14375 120.087595 \nL 245.44375 120.087595 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path id=\"ma1ca5c6cd9\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma1ca5c6cd9\" x=\"50.14375\" y=\"120.087595\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.05 -->\n      <g transform=\"translate(20.878125 123.886814)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path d=\"M 50.14375 88.27649 \nL 245.44375 88.27649 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#ma1ca5c6cd9\" x=\"50.14375\" y=\"88.27649\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 92.075709)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path d=\"M 50.14375 56.465386 \nL 245.44375 56.465386 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#ma1ca5c6cd9\" x=\"50.14375\" y=\"56.465386\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 60.264604)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path d=\"M 50.14375 24.654281 \nL 245.44375 24.654281 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#ma1ca5c6cd9\" x=\"50.14375\" y=\"24.654281\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 28.453499)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <g transform=\"translate(14.798437 86.157813)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 50.14375 13.5 \nL 56.878233 59.925633 \nL 63.612716 83.9177 \nL 70.347198 97.408845 \nL 77.081681 108.111759 \nL 83.816164 115.834658 \nL 90.550647 120.610664 \nL 97.285129 125.558001 \nL 104.019612 128.235446 \nL 110.754095 130.909336 \nL 117.488578 132.499003 \nL 124.22306 133.779215 \nL 130.957543 135.13094 \nL 137.692026 135.634113 \nL 144.426509 136.601675 \nL 151.160991 136.976936 \nL 157.895474 137.557153 \nL 164.629957 138.100581 \nL 171.36444 138.300755 \nL 178.098922 138.101032 \nL 184.833405 138.631683 \nL 191.567888 139.000261 \nL 198.302371 139.201915 \nL 205.036853 139.076631 \nL 211.771336 139.193981 \nL 218.505819 139.140108 \nL 225.240302 139.371276 \nL 231.974784 139.160595 \nL 238.709267 139.5 \nL 245.44375 139.354921 \n\" clip-path=\"url(#p4890d163ef)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 145.8 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 245.44375 145.8 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 145.8 \nL 245.44375 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4890d163ef\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. 预测\n",
    "\n",
    "## 关键思路总结\n",
    "* 输入：源句子→分词→编号→补齐（pad/truncate）→加batch维\n",
    "* 编码器：编码整个输入序列，得到隐藏状态\n",
    "* 解码器：每步用上一步预测作为输入（初始用<BOS>），step by step生成新token\n",
    "* 终止条件：遇到<EOS>或步数用完就停\n",
    "* 最终输出：token id 转字符串"
   ],
   "id": "ef4d309a7fda1687"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测（翻译/生成）\"\"\"\n",
    "    net.eval()  # 评估模式，关掉dropout等\n",
    "\n",
    "    # 1. 源句子编码成token id，并加<eos>\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)  # 有效长度\n",
    "    # 2. 长度不足补pad，多了截断，保证长度一致\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)  # (1, steps)\n",
    "\n",
    "    # 3. 编码器forward，获得隐藏状态\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    # 4. 解码器初始化state\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "\n",
    "    # 5. 解码输入初始为<BOS>\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        # 6. 解码器forward，生成下一个token分布\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 7. 选概率最大的token，作为下一个输入\n",
    "        dec_X = Y.argmax(dim=2)  # (1,1)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()  # 转成int\n",
    "        # 8. 保存注意力（如果需要可视化）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 9. 如果预测到<EOS>，停止生成\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    # 10. 转回字符串\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ],
   "id": "fb3fe221f2a7b44a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. 预测序列的评估\n",
    "\n",
    "## 主要思路\n",
    "- BLEU分数衡量预测和参考答案的“n-gram”重叠程度（翻译/文本生成常用指标）。\n",
    "- 惩罚机制：如果你生成的句子比答案短太多，直接指数级扣分（避免只输出“你好”这种）。\n",
    "- n-gram匹配：统计1-gram、2-gram……k-gram的“命中率”，再用不同权重累乘。\n",
    "- 分数越高表示生成越像答案。"
   ],
   "id": "8569d56ed3be0b2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def bleu(pred_seq, label_seq, k):\n",
    "    \"\"\"计算BLEU分数，评估生成句子的好坏（常用于机器翻译）\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    # 惩罚过短的预测：短得离谱时分数直接低\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    # n-gram累乘打分\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        # 统计label里的所有n-gram短语出现次数\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        # 看pred里的n-gram有多少能在label里找到\n",
    "        for i in range(len_pred - n + 1):\n",
    "            ngram = ' '.join(pred_tokens[i: i + n])\n",
    "            if label_subs[ngram] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[ngram] -= 1\n",
    "        # precision：预测n-gram中有多少匹配\n",
    "        # 权重越高的n越难匹配（通常1-gram和2-gram比重最大）\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score\n"
   ],
   "id": "51f3d4920d0716f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ],
   "id": "74f8898509c12bc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
