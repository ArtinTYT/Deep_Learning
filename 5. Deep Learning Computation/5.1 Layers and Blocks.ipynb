{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 层和块 Layers and Modules\n",
    "\n",
    "nn.Sequential容器将多个层按顺序组合"
   ],
   "id": "4ea51eb9c6d42680"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T07:36:46.305975Z",
     "start_time": "2025-04-23T07:36:46.299120Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# functional: 包含激活函数（如ReLU）、损失函数等函数式操作\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256),  # 输入层：20 → 256\n",
    "                    nn.ReLU(),           # 激活函数\n",
    "                    nn.Linear(256, 10)   # 输出层：256 → 10\n",
    ")\n",
    "\n",
    "X = torch.rand(2, 20)  # 生成形状为(2, 20)的随机输入（批量大小=2，特征数=20）\n",
    "net(X)         # 前向传播，输出形状为(2, 10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0683,  0.1392, -0.2081, -0.0678,  0.2420,  0.0115, -0.1843,  0.1858,\n",
       "         -0.0878, -0.2748],\n",
       "        [-0.0109,  0.1838, -0.2076,  0.0626,  0.1294,  0.1215, -0.0381,  0.2039,\n",
       "         -0.0705, -0.1320]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. 自定义块",
   "id": "2e762be3a5b29705"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:09:11.844519Z",
     "start_time": "2025-04-23T08:09:11.836700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 继承 nn.Module 自定义了一个多层感知机（MLP）模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()                # 初始化父类 nn.Module\n",
    "        self.hidden = nn.Linear(20, 256) # 定义隐藏层（20 → 256）\n",
    "        self.out = nn.Linear(256, 10)    # 定义输出层（256 → 10）\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X))) # 前向传播逻辑"
   ],
   "id": "eff40f9ada9491d7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:09:12.450524Z",
     "start_time": "2025-04-23T08:09:12.443322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = MLP()\n",
    "net(X)"
   ],
   "id": "5af7792a72751005",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1144,  0.1652,  0.0131,  0.0139, -0.2122,  0.0924,  0.1146,  0.0536,\n",
       "          0.1179,  0.0463],\n",
       "        [ 0.0855,  0.2030, -0.1354,  0.1020, -0.1612,  0.0329,  0.0735, -0.0099,\n",
       "          0.0357,  0.0965]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. 顺序块",
   "id": "a6662632e62f5f78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:24:45.214038Z",
     "start_time": "2025-04-23T07:24:45.208687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MySequential(nn.Module):\n",
    "    # 可以传入任意数量的位置参数（即多个模块）：MySequential(module1, module2, module3) 里面作为args 列表\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 使用自增索引作为键，确保唯一性\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ],
   "id": "19d747730d2a2745",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:24:46.015833Z",
     "start_time": "2025-04-23T07:24:46.008940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = MySequential(nn.Linear(20, 256),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(256, 10))\n",
    "net(X)"
   ],
   "id": "d2c4fa50b5827e93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0908,  0.1541, -0.0637, -0.0449,  0.0051, -0.0477, -0.1111, -0.3128,\n",
       "         -0.0575, -0.0918],\n",
       "        [-0.0187,  0.1894,  0.0662, -0.0227,  0.0449, -0.0736, -0.0416, -0.3771,\n",
       "         -0.0382, -0.0785]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. 在前向传播函数中执行代码",
   "id": "169485f9ee47e4e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:26:19.857588Z",
     "start_time": "2025-04-23T07:26:19.853179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)  # 固定权重\n",
    "        self.linear = nn.Linear(20, 20)  # 可学习的全连接层\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)  # 第一次线性变换\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)  # 固定权重的线性变换 + ReLU\n",
    "        X = self.linear(X)  # 第二次线性变换（参数共享）\n",
    "        while X.abs().sum() > 1:  # 动态规范化\n",
    "            X /= 2\n",
    "        return X.sum()  # 输出标量"
   ],
   "id": "b491e797a361a667",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:26:20.411246Z",
     "start_time": "2025-04-23T07:26:20.403087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ],
   "id": "e36f21b9343a3bf8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1705, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 混合搭配各种组合块的方法",
   "id": "cb0b0d08981228dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:27:51.501767Z",
     "start_time": "2025-04-23T07:27:51.490927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 通过组合自定义模块和内置层构建了一个嵌套的神经网络模型 chimera\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(),\n",
    "                        nn.Linear(16, 20),\n",
    "                        FixedHiddenMLP()\n",
    "                        )\n",
    "chimera(X)"
   ],
   "id": "708da03d05bfdb36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0526, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dfd729dfc4bd250e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
